{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cristianbalea/ml-project/blob/main/proiect_balea_ciobanica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rock, Paper, Scissors image classification"
      ],
      "metadata": {
        "id": "AImeW3Oa2Hfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the dataset from Kaggle"
      ],
      "metadata": {
        "id": "Mo0U_V1C2T_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1NjRyW-ztOi"
      },
      "outputs": [],
      "source": [
        "! pip install fastai\n",
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle datasets download -d drgfreeman/rockpaperscissors\n",
        "\n",
        "! unzip rockpaperscissors.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the libraries"
      ],
      "metadata": {
        "id": "TiKMuC8c2ew6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os \n",
        "import fastai\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing import image\n",
        "from fastai.vision.all import *\n",
        "from pathlib import Path\n",
        "import PIL\n",
        "import PIL.Image"
      ],
      "metadata": {
        "id": "8ByPFpSZ2t2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Organizing dataset"
      ],
      "metadata": {
        "id": "a8aryv5b3YwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "build_dir = '/content/rps-cv-images'\n",
        "\n",
        "rock_dir = os.path.join(build_dir, 'rock')\n",
        "paper_dir = os.path.join(build_dir, 'paper')\n",
        "scissors_dir = os.path.join(build_dir, 'scissors')\n",
        "\n",
        "os.listdir(build_dir)"
      ],
      "metadata": {
        "id": "bPFUYLTG3kAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('paper: ', len(os.listdir(paper_dir)))\n",
        "print('scissors: ', len(os.listdir(scissors_dir)))\n",
        "print('rock: ', len(os.listdir(rock_dir)))"
      ],
      "metadata": {
        "id": "II-JIEk14jVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore the dataset"
      ],
      "metadata": {
        "id": "Nr-vRLwlWtL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualisation_dir = Path('../content/rps-cv-images')\n",
        "print(visualisation_dir)\n"
      ],
      "metadata": {
        "id": "5Dia10bxLqUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Images example\n",
        "papers = list(visualisation_dir.glob('paper/*'))\n",
        "rocks = list(visualisation_dir.glob('rock/*'))\n",
        "scissors = list(visualisation_dir.glob('scissors/*'))\n",
        "Image.open(str(papers[10]))\n"
      ],
      "metadata": {
        "id": "agO25g35MDLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image.open(str(rocks[1]))"
      ],
      "metadata": {
        "id": "hqrSmQ1LWHuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image.open(str(scissors[45]))"
      ],
      "metadata": {
        "id": "lH9iFD1zXIU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "njQi3h6BAleB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation"
      ],
      "metadata": {
        "id": "A41nBkB_AqVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an image data generator with specified transformations and preprocessing\n",
        "generator = ImageDataGenerator(\n",
        "    validation_split=0.2,  # Split the data into 80% training and 20% validation\n",
        "    rescale=1/255,  # Rescale pixel values to range between 0 and 1\n",
        "    shear_range=0.2,  # Randomly apply shearing transformations\n",
        "    zoom_range=0.2,  # Randomly apply zooming transformations\n",
        "    rotation_range=90,  # Randomly rotate the images within the range of -20 to 20 degrees\n",
        "    fill_mode='nearest'  # Fill in any newly created pixels using the nearest pixel value\n",
        ")\n",
        "\n",
        "# Create a directory iterator for training data\n",
        "train_data = generator.flow_from_directory(\n",
        "    build_dir,  # Path to the directory containing the training and validation data\n",
        "    batch_size=32,  # Number of images to load and process at each iteration\n",
        "    target_size=(150,150),  # Resize input images to the specified dimensions\n",
        "    subset='training'  # Retrieve images from the training subset of the directory\n",
        ")\n",
        "\n",
        "# Create a directory iterator for validation data\n",
        "val_data = generator.flow_from_directory(\n",
        "    build_dir,  # Path to the directory containing the training and validation data\n",
        "    batch_size=32,  # Number of images to load and process at each iteration\n",
        "    target_size=(150,150),  # Resize input images to the specified dimensions\n",
        "    subset='validation'  # Retrieve images from the validation subset of the directory\n",
        ")"
      ],
      "metadata": {
        "id": "POmsar37_TMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the augmented data"
      ],
      "metadata": {
        "id": "S5Q-ab0lY0H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve a batch of augmented images from the data generator\n",
        "augmented_images = next(train_data)\n",
        "\n",
        "# Display the augmented images\n",
        "fig, axes = plt.subplots(nrows=4, ncols=8, figsize=(12, 6))\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    image = augmented_images[0][i]\n",
        "    ax.imshow(image)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s8n1U0xMYww8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a model"
      ],
      "metadata": {
        "id": "tAJoZZWkDNgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    # Convolutional layer with 32 filters, each of size 3x3.\n",
        "    # ReLU activation function is applied to introduce non-linearity.\n",
        "    # The input shape of the layer is (150, 150, 3), representing RGB images.\n",
        "\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # Max pooling layer with a pool size of 2x2.\n",
        "    # It reduces the spatial dimensions of the previous layer output by taking the maximum value in each pooling region.\n",
        "\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    # Dropout layer with a rate of 0.2.\n",
        "    # It randomly drops 20% of the input units during training to reduce overfitting.\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    # Convolutional layer with 64 filters of size 3x3.\n",
        "    # ReLU activation function is applied to introduce non-linearity.\n",
        "\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Max pooling layer with a pool size of 2x2.\n",
        "\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    # Dropout layer with a rate of 0.2.\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    # Convolutional layer with 128 filters of size 3x3.\n",
        "    # ReLU activation function is applied to introduce non-linearity.\n",
        "\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Max pooling layer with a pool size of 2x2.\n",
        "\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    # Dropout layer with a rate of 0.2.\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    # Convolutional layer with 128 filters of size 3x3.\n",
        "    # ReLU activation function is applied to introduce non-linearity.\n",
        "\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Max pooling layer with a pool size of 2x2.\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # Flatten layer to convert the 2D feature maps to a 1D feature vector.\n",
        "\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Fully connected layer with 512 neurons.\n",
        "    # ReLU activation function is applied.\n",
        "\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    # Dropout layer with a rate of 0.5.\n",
        "\n",
        "    tf.keras.layers.Dense(3, activation=tf.nn.softmax)\n",
        "    # Output layer with 3 neurons, representing the classes of the problem.\n",
        "    # Softmax activation function is applied to generate class probabilities.\n",
        "])"
      ],
      "metadata": {
        "id": "3CTXIWseDPzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "PvMcL0xuZyBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "89PiiyRYDb9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Callbacks(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('accuracy') >= 0.90):\n",
        "            print(\"\\nReached %2.2f%% accuracy, training has been stop\" %(logs.get('accuracy')*100))\n",
        "            self.model.stop_training = True\n",
        "callbacks = Callbacks()\n"
      ],
      "metadata": {
        "id": "NDAelFwQEX81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.optimizers.Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "sIJB1Y68Dfqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "historyModel = model.fit(\n",
        "    train_data,\n",
        "    steps_per_epoch = 25,\n",
        "    epochs = 20,\n",
        "    validation_data = val_data,\n",
        "    validation_steps = 5,\n",
        "    verbose = 2,\n",
        "    callbacks = [callbacks])"
      ],
      "metadata": {
        "id": "1fSNIPB-Ebch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training results"
      ],
      "metadata": {
        "id": "s4svAi5ldsMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(historyModel.history['loss'])\n",
        "plt.plot(historyModel.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(historyModel.history['accuracy'])\n",
        "plt.plot(historyModel.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AhFa1avydze4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = os.path.join(paper_dir,'0Og76sl5CJhbxWWx.png')\n",
        "img = tf.keras.utils.load_img(uploaded, target_size = (150, 150))\n",
        " \n",
        "imgplot = plt.imshow(img)\n",
        "x = tf.keras.utils.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "images = np.vstack([x])\n",
        "\n",
        "classes = model.predict(images, batch_size=10)  \n",
        "if classes[0][0] == 1:\n",
        "  print('Paper')\n",
        "elif classes[0][1] == 1:\n",
        "  print('Rock')\n",
        "else:\n",
        "  print('Scissors')"
      ],
      "metadata": {
        "id": "ScdJH10yEtM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "aloEQaIfFO_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(val_data, steps=val_data.n)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "WKeRkTwrFTlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rock, Paper, Scissor Game"
      ],
      "metadata": {
        "id": "3WQk_0YuLQa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def game(hand_1, hand_2):\n",
        "  img1 = tf.keras.utils.load_img(hand_1, target_size = (150, 150))\n",
        "  \n",
        "  #imgplot1 = plt.imshow(img1)\n",
        "  x1 = tf.keras.utils.img_to_array(img1)\n",
        "  x1 = np.expand_dims(x1, axis=0)\n",
        "  images1 = np.vstack([x1])\n",
        "\n",
        "  classes1 = model.predict(images1, batch_size=10)  \n",
        "\n",
        "  img2 = tf.keras.utils.load_img(hand_2, target_size = (150, 150))\n",
        "  \n",
        "  #imgplot2 = plt.imshow(img2)\n",
        "  x2 = tf.keras.utils.img_to_array(img2)\n",
        "  x2 = np.expand_dims(x2, axis=0)\n",
        "  images2 = np.vstack([x2])\n",
        "\n",
        "  classes2 = model.predict(images2, batch_size=10)  \n",
        "\n",
        "  plt.subplot(1, 2, 1)  # Create a subplot for img1\n",
        "  plt.imshow(img1)\n",
        "  plt.title('Image 1')\n",
        "  plt.xlabel(f'Class: {np.argmax(classes1)}')\n",
        "\n",
        "  plt.subplot(1, 2, 2)  # Create a subplot for img2\n",
        "  plt.imshow(img2)\n",
        "  plt.title('Image 2')\n",
        "  plt.xlabel(f'Class: {np.argmax(classes2)}')\n",
        "\n",
        "  plt.tight_layout()  # Adjust spacing between subplots\n",
        "  plt.show()\n",
        "\n",
        "  if classes1[0][0] == 1:\n",
        "    h1 = 'paper'\n",
        "  elif classes1[0][1] == 1:\n",
        "    h1 = 'rock'\n",
        "  else:\n",
        "    h1 = 'scissors'\n",
        "\n",
        "\n",
        "  if classes2[0][0] == 1:\n",
        "    h2 = 'paper'\n",
        "  elif classes2[0][1] == 1:\n",
        "    h2 = 'rock'\n",
        "  else:\n",
        "    h2 = 'scissors'\n",
        "\n",
        "\n",
        "  print(\"h1: \", h1)\n",
        "  print(\"h2: \", h2)\n",
        "\n",
        "\n",
        "  if h1 == 'paper':\n",
        "    if h2 == 'rock':\n",
        "      print(\"hand 1 won\")\n",
        "    elif h2 == 'paper':\n",
        "      print(\"draw\")\n",
        "    else:\n",
        "      print(\"hand 2 won\")\n",
        "\n",
        "  if h1 == 'rock':\n",
        "    if h2 == 'paper':\n",
        "      print(\"hand 2 won\")\n",
        "    elif h2 == 'rock':\n",
        "      print(\"draw\")\n",
        "    else:\n",
        "      print(\"hand 1 won\")\n",
        "\n",
        "  if h1 == 'scissors':\n",
        "    if h2 == 'rock':\n",
        "      print(\"hand 2 won\")\n",
        "    elif h2 == 'scissors':\n",
        "      print(\"draw\")\n",
        "    else:\n",
        "      print(\"hand 1 won\")"
      ],
      "metadata": {
        "id": "spY2cg16LU4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hand_1 = '/content/rps-cv-images/paper/0a3UtNzl5Ll3sq8K.png'\n",
        "hand_2 = '/content/rps-cv-images/scissors/2M8LvUBGMOH1bsaz.png'\n",
        "\n",
        "game(hand_1, hand_2)"
      ],
      "metadata": {
        "id": "9pYD-SHDPaJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}